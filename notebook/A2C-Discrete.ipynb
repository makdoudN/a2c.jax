{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b783b89d-f31b-4cbf-9888-27d9b33b03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import jax\n",
    "import tax\n",
    "import tree\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "\n",
    "from a2c.a2c import Data\n",
    "from a2c.a2c import State\n",
    "from a2c.a2c import Batch\n",
    "from a2c.a2c import process_data\n",
    "from a2c.a2c import loss_fn\n",
    "from a2c.common.nn import mlp_categorical\n",
    "from a2c.common.nn import mlp_deterministic\n",
    "from jax import jit\n",
    "from jax import vmap\n",
    "from gym.vector import AsyncVectorEnv\n",
    "from functools import partial\n",
    "\n",
    "tax.set_platform('cpu')\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "\n",
    "def evaluation(rng, env, policy, niters: int = 5):\n",
    "    all_scores = []\n",
    "    for _ in range(niters):\n",
    "        observation, score = env.reset(), 0\n",
    "        for _ in range(env.spec.max_episode_steps):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = policy(rng, observation)\n",
    "            action = int(action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if done: \n",
    "                break\n",
    "        all_scores.append(score)\n",
    "    info = {}\n",
    "    info['eval/score'] = np.mean(all_scores)\n",
    "    info['eval/score_std'] = np.std(all_scores)\n",
    "    return info\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(2, 3, 4))\n",
    "def update_fn(state, inputs, policy_opt, value_opt, loss_fn):\n",
    "    \"\"\" Generic Update function \"\"\"\n",
    "    g, metrics = jax.grad(loss_fn, has_aux=True)(state.params, inputs)\n",
    "    \n",
    "    # -- Value Update\n",
    "    updates, value_opt_state = value_opt(g['value'], state.opt_state['value'])\n",
    "    value_params = jax.tree_multimap(lambda p, u: p + u, state.params['value'], updates)\n",
    "    # -- Policy Update\n",
    "    updates, policy_opt_state = policy_opt(g['policy'], state.opt_state['policy'])\n",
    "    policy_params = jax.tree_multimap(lambda p, u: p + u, state.params['policy'], updates)\n",
    "    \n",
    "    params = state.params\n",
    "    params = dict(policy=policy_params, value=value_params)\n",
    "    opt_state = state.opt_state\n",
    "    opt_state = dict(policy=policy_opt_state, value=value_opt_state)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "# TODO Add discount, lambda reward_scaling -> `process_data_to_batch`\n",
    "def init_nn(\n",
    "    observation_size, action_size, seed=42,\n",
    "    policy_opt = 'adabelief',\n",
    "    policy_opt_kwargs = dict(learning_rate=5e-3),\n",
    "    value_opt = 'adabelief',\n",
    "    value_opt_kwargs = dict(learning_rate=5e-3),\n",
    "    policy_kwargs: dict = dict(), value_kwargs: dict = dict(),\n",
    "):\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    dummy_action = jnp.zeros((action_size))\n",
    "    dummy_observation = jnp.zeros((observation_size))\n",
    "    \n",
    "    policy_def = lambda x: mlp_categorical(action_size, **policy_kwargs)(x)\n",
    "    policy_def = hk.transform(policy_def)\n",
    "    policy_def = hk.without_apply_rng(policy_def)\n",
    "    policy_opt = getattr(optax, policy_opt)(**policy_opt_kwargs)\n",
    "    \n",
    "    value_def = lambda x: mlp_deterministic(1, **value_kwargs)(x).squeeze(-1)\n",
    "    value_def = hk.transform(value_def)\n",
    "    value_def = hk.without_apply_rng(value_def)\n",
    "    value_opt = getattr(optax, value_opt)(**value_opt_kwargs)\n",
    "    \n",
    "    rng, rng_policy, rng_value = jax.random.split(rng, 3)\n",
    "    value_params = value_def.init(rng_policy, dummy_observation)\n",
    "    value_opt_state = value_opt.init(value_params)\n",
    "    policy_params = policy_def.init(rng_policy, dummy_observation)\n",
    "    policy_opt_state = policy_opt.init(policy_params)\n",
    "\n",
    "    params = {'policy': policy_params, 'value': value_params}\n",
    "    opt_state = {'policy': policy_opt_state, 'value': value_opt_state}\n",
    "    \n",
    "    process_data_to_batch = partial(\n",
    "        process_data, value_apply=value_def.apply,    \n",
    "    )\n",
    "    \n",
    "    loss = partial(loss_fn, value_apply=value_def.apply, policy_apply=policy_def.apply)\n",
    "    update = partial(update_fn, \n",
    "                     policy_opt=policy_opt.update, \n",
    "                     value_opt=value_opt.update,\n",
    "                     loss_fn=loss)    \n",
    "    \n",
    "    \n",
    "    def make_policy(state):\n",
    "        fn = lambda rng, x: policy_def.apply(state.params['policy'], x).sample(seed=rng) \n",
    "        return jit(fn)\n",
    "    \n",
    "    return State(key=rng, params=params, opt_state=opt_state), {\n",
    "        'process_data': jit(process_data_to_batch),\n",
    "        'make_policy': make_policy,\n",
    "        'loss': jit(loss), 'update': jit(update),\n",
    "        'full_update': jit(lambda state, data: update(state, process_data_to_batch(state.params, data)))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83467320-19a9-4a49-8ce4-dd54e0461ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEnvs = 8\n",
    "make_env = lambda: gym.make('CartPole-v0')\n",
    "env = AsyncVectorEnv([make_env for _ in range(NEnvs)])\n",
    "env_test = gym.make('CartPole-v0')\n",
    "action_size = env_test.action_space.n\n",
    "observation_size = env_test.observation_space.shape[0]\n",
    "state, a2c = init_nn(observation_size, action_size, 42, policy_kwargs={}, value_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f067edbc-a280-49fe-8497-216afcf894fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction(env, horizon: int = 10, seed: int = 42):\n",
    "    # -- Initialization\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    observation, buf = env.reset(), []\n",
    "    policy = yield\n",
    "    \n",
    "    # -- Interaction Loop.\n",
    "    while True:\n",
    "        for _ in range(horizon):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = np.array(policy(rng_action, observation))\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            buf.append({\n",
    "                'observation': observation,\n",
    "                'reward': reward,\n",
    "                'terminal': 1. - done,\n",
    "                'action': action\n",
    "            })\n",
    "            observation = observation_next.copy()\n",
    "            \n",
    "        data = tax.reduce(buf)\n",
    "        data['last_observation'] = observation\n",
    "        policy = yield data\n",
    "        buf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953fcfa1-5ce8-4267-8ffe-099fc7934ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_step = interaction(env, 10)\n",
    "interaction_step.send(None)\n",
    "policy = a2c['make_policy'](state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04965b26-03af-4254-af94-4e96b0f154d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    data = interaction_step.send(policy)\n",
    "    state, info_fit = a2c['full_update'](state, data)\n",
    "    policy = a2c['make_policy'](state) \n",
    "    interaction_step.send(policy)\n",
    "    info_eval = evaluation(rng, env_test, policy)\n",
    "    info = {}\n",
    "    info.update(**info_fit, **info_eval)\n",
    "    info = tree.map_structure(lambda v: float(v), info)\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b6784-ba98-43ed-bbf0-35f80cd74afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(rng, env_test, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2365b2-80f2-42b2-8611-c8f9e0ff5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy(rng, env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ef3fc-09b8-4fca-adaa-8db40ff665eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.params['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cac040-a320-4046-826a-e12aec4f2731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
